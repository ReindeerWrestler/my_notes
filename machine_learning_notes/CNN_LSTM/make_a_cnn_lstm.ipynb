{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from numpy import array\n",
    "from numpy import split\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the following to set up the data in the correct format. Explained in data_setup under General"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def chunk_and_split_data_to_x_y(df, chunk_size, expected_cols):\n",
    "    \"\"\"\n",
    "    Built to replace both chunk_data and split_data_to_x_y. Does the exact same job but now with a lot less steps.\n",
    "\n",
    "    Takes in a dataframe whose data you want to use to train a model with. Removes the expected data output from the training dataset.\n",
    "    Sets up the expected data in the correct format and sets up the training data in the correct format\n",
    "    Mainly used to organize data for training with a neural network such as keras\n",
    "\n",
    "    :param df: A dataframe\n",
    "    :param chunk_size: The size of the 2D matrix that you want to use to train the model. Can handle any non-negative int lower in size than the input dataframe\n",
    "    :param expected_cols: The expected output columns that are the correct answer for the model. Aka your y\n",
    "    :return: training data and expected data\n",
    "    \"\"\"\n",
    "    balance_num = df.shape[0] % chunk_size\n",
    "    expected = df.loc[:, expected_cols]\n",
    "    expected = expected[chunk_size + balance_num:].values\n",
    "    expected = expected.reshape(expected.shape[0], 1)\n",
    "    df = df.drop(columns=expected_cols)\n",
    "    if balance_num == 0:\n",
    "        training_data = df.values\n",
    "        training_data = np.array(np.split(training_data, len(training_data)/chunk_size))\n",
    "    else:\n",
    "        df = df.drop(df.index[range(balance_num)])\n",
    "        training_data = df.values\n",
    "        training_data = np.array(np.split(training_data, len(training_data)/chunk_size))\n",
    "\n",
    "    return training_data, expected\n",
    "\n",
    "df = pd.read_csv(\"/home/nelson/PycharmProjects/my_notes/Dummy_data_for_examples/flagstaff_solar_data.csv\", index_col=\"date_time\", parse_dates=True)\n",
    "df_val = pd.read_csv(\"/home/nelson/PycharmProjects/my_notes/Dummy_data_for_examples/flagstaff_val_solar_data.csv\", index_col=\"date_time\", parse_dates=True)\n",
    "# data is already setup from another project\n",
    "\n",
    "\n",
    "input_chunk_size = 20\n",
    "expected_data_output_col = \"expected_solar\"\n",
    "\n",
    "training_input_data, training_output_data = chunk_and_split_data_to_x_y(df, chunk_size=input_chunk_size,\n",
    "                                                                        expected_cols=expected_data_output_col)\n",
    "\n",
    "val_input_data, val_output_data = chunk_and_split_data_to_x_y(df, chunk_size=input_chunk_size,\n",
    "                                                                        expected_cols=expected_data_output_col)\n",
    "\n",
    "LAG = 20 # this is the setting for the sliding window of data that the model views\n",
    "EPOCHS = 10\n",
    "VERBOSE = 1\n",
    "BATCH_SIZE = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now to build the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def build_model(train_x, train_y, val=None, epochs=10, verbose=1, batch_size=32, activation='relu', loss='mse', optimizer='adam',\n",
    "                save_model_file_path=os.getcwd(), loss_monitor='val_loss'):\n",
    "    \"\"\"\n",
    "    A simple CNN-LSTM that is designed to be used as either an example for how to build a CNN-LSTM or as a ready-made model to test a CNN-LSTM\n",
    "\n",
    "    :param optimizer: optimizer that the model uses\n",
    "    :param loss: loss metric\n",
    "    :param activation: activation function\n",
    "    :param loss_monitor: method of monitoring the loss\n",
    "    :param save_model_file_path: path to save the current best performing model\n",
    "    :param train_x: features that will be used to train the lstm\n",
    "    :param train_y: expected output\n",
    "    :param val: validation data\n",
    "    :param epochs: number of epochs to be used to train the model\n",
    "    :param verbose: controller for keras if output should be given as the model is trained. Set to true\n",
    "    :param batch_size: Batch size for the training data.\n",
    "    :return: the trained model and the model history.\n",
    "    \"\"\"\n",
    "    # define parameters\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    # train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation=activation, input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Conv1D(64, 3, activation=activation))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(200, activation=activation, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation=activation)))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    filepath = save_model_file_path + \"-weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5.\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=loss_monitor, verbose=1, save_best_only=True, mode='min')\n",
    "    es = EarlyStopping(monitor=loss_monitor, mode='min', verbose=1, patience=5)\n",
    "    callbacks_list = [checkpoint, es]\n",
    "    # fit network\n",
    "    history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,\n",
    "              verbose=verbose, validation_data=val, callbacks=callbacks_list)\n",
    "    return model, history\n",
    "\n",
    "model, history = build_model(training_input_data, training_output_data, val=(val_input_data, val_output_data), epochs=EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}