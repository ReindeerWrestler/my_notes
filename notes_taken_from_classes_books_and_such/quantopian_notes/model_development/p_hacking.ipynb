{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Data mining for fit. The bane of the industery\n",
    "\n",
    "Point in this is to give more information about the problem and when it may be occuring\n",
    "\n",
    "Treat p-vals as binary\n",
    "\n",
    "Runs p val tests on all columns of a dataframe compared to one another\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can't really eliminate multiple comparisons bias, but you can reduce how much it impacts you. To do so we have two options.\n",
    "\n",
    " Run fewer tests. This is often the best option. Rather than just sweeping around hoping you hit an interesting signal, use your expert knowledge of the system to develop a great hypothesis and test that.\n",
    "\n",
    "Adjustment Factors and Bon Ferroni Correction: If you must run many tests, try to correct your p-values. This means applying a correction factor to the cutoff you desire to obtain the one actually used when determining whether p-values are significant. The most conservative and common correction factor is Bon Ferroni.\n",
    "\n",
    "Because Bon Ferroni is so stringent, you can often end up passing over real relationships. There is a good example in the following article"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.00025"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in Bon Ferroni you change the p cutoff value to\n",
    "confidence_percentage = 0.05\n",
    "number_of_tests = 100\n",
    "\n",
    "new_cut_off_val = confidence_percentage / number_of_tests\n",
    "# this being so low is why you can lose real relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because Bon Ferroni is so stringent, you can often end up passing over real relationships. There is a good example in the following article\n",
    "\n",
    "Effectively, it assumes that all the tests you are running are independent, and doesn't take into account any structure in your data. You may be able to design a more finely tuned correction factor, but this is adding a layer of complexity and therefore a point of failure to your research. In general any time you relax your stringency, you need to be very careful not to make a mistake.\n",
    "\n",
    "Because of the over-zealousness of Bon Ferroni, often running fewer tests is the better option. Or, if you must run many tests, reserve multiple sets of data so your candidate signals can undergo an out-of-sample round of testing.\n",
    "\n",
    "In general you can never completely eliminate multiple comparisons bias, you can only reduce the risk of false positives using techniques we described above. At the end of the day most ideas tried in research don't work, so you'll end up testing many different hypotheses over time. Just try to be careful and use common sense about whether there is sufficient evidence that a hypothesis is true, or that you just happened to get lucky on this iteration."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
